{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning -- Convolutional Neural Network (CNN) for Handwritten Digits Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "Follow instructions step by step until the end and submit your complete notebook as an archive (`tar -cf groupXnotebook.tar DL_lab2/`).\n",
    "\n",
    "Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by the deadline on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lab session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST dataset. The best achieved accuracy on testing data was >90%.  Can  you do better than these results using a deep CNN?\n",
    "Now, let's build a more interesting model. In this notebook, you will build, train and optimize in TensorFlow one of the simplest Convolutional Neural Networks, **LeNet-5**, proposed by Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner in 1998 (for more details, check the paper *\"Gradient-Based Learning Applied to Document Recognition\"*, Y.LeCun et al.).\n",
    "\n",
    "\n",
    "Since, it's possible that this is our first time using Tensorflow, the next section will serve as introduction to this framework (*Note*: we will use TF 1.x).\n",
    "**Please, even if there is no coding required -- do NOT skip it and READ carefully everything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-7cb319862cb8>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow 101\n",
    "## TensorFlow Static Graph\n",
    "\n",
    "The entire purpose of Tensorflow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can be more efficient than NumPy because TensorFlow knows the entire computation graph that must be executed, while NumPy only knows the computation of a single mathematical operation at a time.\n",
    "\n",
    "TensorFlow can also automatically calculate the gradients that are needed to optimize the variables of the graph so as to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain-rule for derivatives.\n",
    "\n",
    "TensorFlow can also take advantage of multi-core CPUs as well as GPUs - and Google has even built special hardware accelerators just for TensorFlow which are called TPUs (Tensor Processing Units) that are even faster than GPUs.\n",
    "\n",
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "- Placeholder variables used to feed input into the graph.\n",
    "- Model variables that are going to be optimized so as to make the model perform better.\n",
    "- The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
    "- A cost measure that can be used to guide the optimization of the variables.\n",
    "- An optimization method which updates the variables of the model.\n",
    "\n",
    "In addition, the TensorFlow graph may also contain various debugging statements e.g. for logging data to be displayed using TensorBoard.\n",
    "\n",
    "## Placeholder variables\n",
    "Placeholder variables serve as the input to the graph that we may change each time we execute the graph. We call this feeding the placeholder variables and it is demonstrated further below.\n",
    "\n",
    "First we define the placeholder variable for the input images. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional vector or matrix. The data-type is set to float32 and the shape is set to `[None, img_size_flat]`, where None means that the tensor may hold an arbitrary number of images with each image being a vector of length `img_size_flat` (in our case it's 784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(?, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name='inputs')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable x. \n",
    "The shape of this placeholder variable is `[None, num_classes]` which means it may hold an arbitrary number of labels and each label is a vector of length `num_classes` which is 10 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"labels:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have the tensor variable for the true class of each image in the placeholder variable `x`. These are integers and the dimensionality of this placeholder variable is set to `[None]` which means the placeholder variable is a one-dimensional vector of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the placeholder variables that were defined above and which serve as feeding input data into the model, there are also some model variables that must be changed by TensorFlow so as to make the model perform better on the training data.\n",
    "\n",
    "The first variable that must be optimized is called `weights` and is defined here as a TensorFlow variable that must be initialized with zeros and whose shape is `[img_size_flat, num_classes]`, so it is a 2-dimensional tensor (or matrix) with `img_size_flat` rows and `num_classes` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weights:0' shape=(784, 10) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.zeros([784, 10]), name='weights')\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second variable that must be optimized is called `biases` and is defined as a 1-dimensional tensor (or vector) of length `num_classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "biases = tf.Variable(tf.zeros([10]), name='bias')\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple mathematical model multiplies the images in the placeholder variable `x` with the `weights` and then adds the `biases`.\n",
    "\n",
    "The result is a matrix of shape `[num_images, num_classes]` because `x` has shape `[num_images, img_size_flat]` and `weights` has shape `[img_size_flat, num_classes]`, so the multiplication of those two matrices is a matrix with shape `[num_images, num_classes]` and then the `biases` vector is added to each row of that matrix.\n",
    "\n",
    "Note that the name `logits` is typical TensorFlow terminology, but other people may call the variable something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `logits` is a matrix with `num_images` rows and `num_classes` columns, where the element of the $i$'th row and $j$'th column is an estimate of how likely the $i$'th input image is to be of the $j$'th class.\n",
    "\n",
    "However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each row of the `logits` matrix sums to one, and each element is limited between zero and one. This is calculated using the so-called softmax function and the result is stored in `y_pred`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted class can be calculated from the `y_pred` matrix by taking the index of the largest element in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('model'):\n",
    "    logits = tf.matmul(x, weights) + biases\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-function to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model better at classifying the input images, we must somehow change the variables for `weights` and `biases`. To do this we first need to know how well the model currently performs by comparing the predicted output of the model `y_pred` to the desired output `y_true`.\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is therefore to minimize the cross-entropy so it gets as close to zero as possible by changing the `weights` and `biases` of the model.\n",
    "\n",
    "TensorFlow has a built-in function for calculating the cross-entropy. Note that it uses the values of the `logits` because it also calculates the softmax internally.\n",
    "\n",
    "After that, we have the cross-entropy for each of the image classifications so we have a measure of how well the model performs on each image individually. But in order to use the cross-entropy to guide the optimization of the model's variables we need a single scalar value, so we simply take the average of the cross-entropy for all the image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                               labels=y_true)\n",
    "    loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Now that we have a cost measure that must be minimized, we can then create an optimizer. In this case it is the basic form of Gradient Descent where the step-size is set to 0.01.\n",
    "\n",
    "Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope('optim'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    opt_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few more performance measures to display the progress to the user.\n",
    "\n",
    "This is a vector of booleans whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True becomes 1, and then calculating the average of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session\n",
    "\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables\n",
    "The variables for `weights` and `biases` must be initialized before we start optimizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the TensorBoard\n",
    "\n",
    "Tensorboard is shipped with TensorFlow and it's a tool that allows to plot metrics, debug the graph, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_path(path_pattern):\n",
    "    import os\n",
    "    i = 1\n",
    "    while os.path.exists(path_pattern % i):\n",
    "        i = i * 2\n",
    "    a, b = (i / 2, i)\n",
    "    while a + 1 < b:\n",
    "        c = (a + b) / 2 \n",
    "        a, b = (c, b) if os.path.exists(path_pattern % c) else (a, c)\n",
    "    directory = path_pattern % b\n",
    "    return directory\n",
    "\n",
    "\n",
    "writer = tf.summary.FileWriter(next_path('logs/run_%02d'))\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is defined, we can move to running the optimization.\n",
    "There are 55.000 images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore use Stochastic Gradient Descent which only uses a small batch of images in each iteration of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for performing a number of optimization iterations so as to gradually improve the `weights` and `biases` of the model. In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples. \n",
    "\n",
    "Let's define a couple of functions that will be usefull later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(epochs):\n",
    "    # Go through the traning dataset `epochs` times\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        num_of_batches = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # We save also the loss across all the batches of data for \n",
    "        # presentation purpose\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(num_of_batches):\n",
    "            # Get a batch of training examples (shuffle every epoch).\n",
    "            # x_batch now holds a batch of images and\n",
    "            # y_true_batch are the true labels for those images.\n",
    "            x_batch, y_true_batch = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            \n",
    "            # Put the batch into a dict with the proper names\n",
    "            # for placeholder variables in the TensorFlow graph.\n",
    "            # Note that the placeholder for y_true_cls is not set\n",
    "            # because it is not used during training.\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                               y_true: y_true_batch}\n",
    "            \n",
    "            # Run the optimizer using this batch of training data.\n",
    "            # TensorFlow assigns the variables in feed_dict_train\n",
    "            # to the placeholder variables and then runs the optimizer.\n",
    "            session.run(opt_step, feed_dict=feed_dict_train)\n",
    "            \n",
    "            # Similarly, get the loss and accuracy metrics on the batch of data\n",
    "            batch_loss, summary = session.run([loss, merged_summary_op], feed_dict=feed_dict_train)\n",
    "            \n",
    "            # Write logs at every iteration\n",
    "            writer.add_summary(summary, e * num_of_batches + i)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_loss += batch_loss / num_of_batches\n",
    "            \n",
    "        print(\"Epoch: \", '%02d' % (e + 1), \"  =====> Loss =\", \"{:.6f}\".format(avg_loss))\n",
    "\n",
    "def print_accuracy():\n",
    "    feed_dict_test = {x: mnist.test.images,\n",
    "                  y_true: mnist.test.labels}\n",
    "    \n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    # We are also going to save some metric like memory usage and computation time\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test, options=run_options, run_metadata=run_metadata)\n",
    "    try:\n",
    "        writer.add_run_metadata(run_metadata, 'inference')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    writer.flush()\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the graph to TensorBoard for easy debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance before any optimization\n",
    "\n",
    "The accuracy on the test-set is 9.8%. This is because the model has only been initialized and not optimized at all, so it always predicts that the image shows a zero digit and it turns out that 9.8% of the images in the test-set happens to be zero digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 9.8%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model for 50 epochs and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss = 1.176233\n",
      "Epoch:  02   =====> Loss = 0.662504\n",
      "Epoch:  03   =====> Loss = 0.550655\n",
      "Epoch:  04   =====> Loss = 0.496804\n",
      "Epoch:  05   =====> Loss = 0.463772\n",
      "Epoch:  06   =====> Loss = 0.440938\n",
      "Epoch:  07   =====> Loss = 0.423967\n",
      "Epoch:  08   =====> Loss = 0.410685\n",
      "Epoch:  09   =====> Loss = 0.399935\n",
      "Epoch:  10   =====> Loss = 0.390951\n",
      "Epoch:  11   =====> Loss = 0.383337\n",
      "Epoch:  12   =====> Loss = 0.376758\n",
      "Epoch:  13   =====> Loss = 0.371061\n",
      "Epoch:  14   =====> Loss = 0.365974\n",
      "Epoch:  15   =====> Loss = 0.361400\n",
      "Epoch:  16   =====> Loss = 0.357319\n",
      "Epoch:  17   =====> Loss = 0.353593\n",
      "Epoch:  18   =====> Loss = 0.350193\n",
      "Epoch:  19   =====> Loss = 0.347047\n",
      "Epoch:  20   =====> Loss = 0.344152\n",
      "Epoch:  21   =====> Loss = 0.341471\n",
      "Epoch:  22   =====> Loss = 0.338952\n",
      "Epoch:  23   =====> Loss = 0.336701\n",
      "Epoch:  24   =====> Loss = 0.334475\n",
      "Epoch:  25   =====> Loss = 0.332478\n",
      "Epoch:  26   =====> Loss = 0.330563\n",
      "Epoch:  27   =====> Loss = 0.328733\n",
      "Epoch:  28   =====> Loss = 0.327004\n",
      "Epoch:  29   =====> Loss = 0.325350\n",
      "Epoch:  30   =====> Loss = 0.323827\n",
      "Epoch:  31   =====> Loss = 0.322352\n",
      "Epoch:  32   =====> Loss = 0.320978\n",
      "Epoch:  33   =====> Loss = 0.319624\n",
      "Epoch:  34   =====> Loss = 0.318352\n",
      "Epoch:  35   =====> Loss = 0.317105\n",
      "Epoch:  36   =====> Loss = 0.315922\n",
      "Epoch:  37   =====> Loss = 0.314849\n",
      "Epoch:  38   =====> Loss = 0.313730\n",
      "Epoch:  39   =====> Loss = 0.312709\n",
      "Epoch:  40   =====> Loss = 0.311709\n",
      "Epoch:  41   =====> Loss = 0.310771\n",
      "Epoch:  42   =====> Loss = 0.309773\n",
      "Epoch:  43   =====> Loss = 0.308890\n",
      "Epoch:  44   =====> Loss = 0.308054\n",
      "Epoch:  45   =====> Loss = 0.307176\n",
      "Epoch:  46   =====> Loss = 0.306363\n",
      "Epoch:  47   =====> Loss = 0.305632\n",
      "Epoch:  48   =====> Loss = 0.304744\n",
      "Epoch:  49   =====> Loss = 0.304122\n",
      "Epoch:  50   =====> Loss = 0.303344\n",
      "Accuracy on test-set: 91.8%\n"
     ]
    }
   ],
   "source": [
    "optimize(50)\n",
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard, we can now visualize the created graph, giving you an overview of your architecture and how all of the major components are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch TensorBoard: \n",
    "- Open a Terminal and run the command line `tensorboard --logdir logs`\n",
    "- Click on \"Tensorboard web interface\" in Zoe (available only within EURECOM network) \n",
    "\n",
    "Try to play with it: check the behaviour of the training procedure, check the graph and try to match the nodes with their definition in the code, check the profiling that we did for the inference (how much memory it requires, how long it takes, ...).\n",
    "\n",
    "<b> I unfortunatly can't use TensorBoard even with the instructions given... There would not be any graph, sorry. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\" width=\"800\" height=\"600\" align=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0., shape=shape)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def build_lenet5(x):\n",
    "    with tf.name_scope(\"reshape\"):\n",
    "        image = tf.reshape(x, [-1, 28, 28, 1]) # [None, 28, 28, 1]\n",
    "    \n",
    "    with tf.name_scope(\"lenet5\"):\n",
    "        with tf.name_scope(\"layer1\"):\n",
    "           ## --- COMPLETE --- ##\n",
    "            conv1_W = weight_variable((5, 5, 1, 6), \"conv1_W\")\n",
    "            conv1_b = bias_variable([1, 28, 28, 6], \"bias1\")\n",
    "            conv1 = tf.nn.conv2d(input = image, filter = conv1_W, strides = [1, 1, 1, 1], padding = 'SAME') + conv1_b\n",
    "            relu1 = tf.nn.relu(conv1)\n",
    "            max_pool1 = tf.nn.max_pool(value = relu1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"layer2\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            conv2_W = weight_variable((5, 5, 6, 16), \"conv2_W\")\n",
    "            conv2_b = bias_variable([1, 10, 10, 16], \"bias2\")\n",
    "            conv2 = tf.nn.conv2d(input = max_pool1, filter = conv2_W, strides = [1, 1, 1, 1], padding = 'VALID') + conv2_b\n",
    "            relu2 = tf.nn.relu(conv2)\n",
    "            max_pool2 = tf.nn.max_pool(value = relu2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"flatten\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            flat = flatten(max_pool2)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"layer3\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            layer3_W = weight_variable((400, 120), \"layer3_W\")\n",
    "            layer3_b = tf.Variable(tf.zeros([120]), name='bias3')\n",
    "            fc3 = tf.matmul(flat, layer3_W) + layer3_b\n",
    "            o_layer3 = tf.nn.relu(fc3)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"layer4\"):\n",
    "            ## --- COMPLETE --- ##    \n",
    "            layer4_W = weight_variable((120, 84), \"layer4_W\")\n",
    "            layer4_b = tf.Variable(tf.zeros([84]), name='bias4')\n",
    "            fc4 = tf.matmul(o_layer3, layer4_W) + layer4_b\n",
    "            o_layer4 = tf.nn.relu(fc4)\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"layer5\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            layer5_W = weight_variable((84, 10), \"layer5_W\")\n",
    "            layer5_b = tf.Variable(tf.zeros([10]), name='bias5')            \n",
    "            fc5 = tf.matmul(o_layer4, layer5_W) + layer5_b\n",
    "\n",
    "    \n",
    "    return fc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question  </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters can be calculating thanks to the caracteristics of the layers we have (we calculate the parameters of the convolution layers, activations layer, pooling layers and Fully Connected layers), by additionning the number of wieghts and the number of biases: \n",
    "    * Input layer: 0 parameter\n",
    "    * Layer 1: 5x5x1x6 + 6 = 156 parameters\n",
    "    * Layer 2: 5x5x6x16 + 16 = 2 416 parameters\n",
    "    * Layer 3: 400x120 + 120 =  48 120 parameters\n",
    "    * Layer 4: 120x84 + 84 = 10 164 parameters\n",
    "    * Layer 5: 84x10 + 10 = 850 parameters\n",
    "In adiddition, the total number of parameters is <b> 61 706 parameters </b> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='inputs')\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "y_true_cls = tf.argmax(y_true, 1) \n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    logits = build_lenet5(x)   ## --- COMPLETE --- ##\n",
    "    y_pred = tf.nn.softmax(logits)   ## --- COMPLETE --- ##\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)   ## --- COMPLETE --- ##\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_true)   ## --- COMPLETE --- ##\n",
    "    loss = tf.reduce_mean(cross_entropy)   ## --- COMPLETE --- ##\n",
    "    \n",
    "with tf.name_scope('optim'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)   ## --- COMPLETE --- ##\n",
    "    #optimizer2 = tf.train.GradientDescentOptimizer(learning_rate)   ## --- COMPLETE --- ##\n",
    "    opt_step = optimizer.minimize(loss)   ## --- COMPLETE --- ##\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)   ## --- COMPLETE --- ##\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   ## --- COMPLETE --- ##\n",
    "\n",
    "session = tf.Session()   ## --- COMPLETE --- ##\n",
    "init_op = tf.global_variables_initializer()   ## --- COMPLETE --- ##\n",
    "session.run(init_op)\n",
    "\n",
    "writer = tf.summary.FileWriter(next_path('logs/lenet5/run_%02d'))\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- COMPLETE --- ##\n",
    "\n",
    "def optimize2(epochs):\n",
    "    # Go through the traning dataset `epochs` times\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        num_of_batches = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # We save also the loss across all the batches of data for \n",
    "        # presentation purpose\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(num_of_batches):\n",
    "            # Get a batch of training examples (shuffle every epoch).\n",
    "            # x_batch now holds a batch of images and\n",
    "            # y_true_batch are the true labels for those images.\n",
    "            x_batch, y_true_batch = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            \n",
    "            # Put the batch into a dict with the proper names\n",
    "            # for placeholder variables in the TensorFlow graph.\n",
    "            # Note that the placeholder for y_true_cls is not set\n",
    "            # because it is not used during training.\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                               y_true: y_true_batch}\n",
    "\n",
    "            # Run the optimizer using this batch of training data.\n",
    "            # TensorFlow assigns the variables in feed_dict_train\n",
    "            # to the placeholder variables and then runs the optimizer.\n",
    "            session.run(opt_step, feed_dict=feed_dict_train)\n",
    "            \n",
    "            # Similarly, get the loss and accuracy metrics on the batch of data\n",
    "            batch_loss, summary, acc_train = session.run([loss, merged_summary_op, accuracy], feed_dict=feed_dict_train)\n",
    "            \n",
    "            # Write logs at every iteration\n",
    "            writer.add_summary(summary, e * num_of_batches + i)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_loss += batch_loss / num_of_batches\n",
    "            \n",
    "            \n",
    "            feed_dict_validation = {x: mnist.validation.images,\n",
    "                  y_true: mnist.validation.labels}\n",
    "            acc_validation = session.run(accuracy, feed_dict=feed_dict_validation)\n",
    "\n",
    "\n",
    "        print(\"Epoch: \", '%02d' % (e + 1), \"  =====> Loss =\", \"{:.6f}\".format(avg_loss), \n",
    "             \", Training Accuracy = \", acc_train, \n",
    "              \", Validation Accuracy = \", acc_validation)\n",
    "\n",
    "def print_accuracy2():\n",
    "    feed_dict_test = {x: mnist.test.images,\n",
    "                  y_true: mnist.test.labels}\n",
    "    \n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    # We are also going to save some metric like memory usage and computation time\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test, options=run_options, run_metadata=run_metadata)\n",
    "    try:\n",
    "        writer.add_run_metadata(run_metadata, 'inference')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    writer.flush()\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss = 2.301843 , Training Accuracy =  0.1484375 Validation Accuracy =  0.1628\n",
      "Epoch:  02   =====> Loss = 2.295971 , Training Accuracy =  0.25 Validation Accuracy =  0.1978\n",
      "Epoch:  03   =====> Loss = 2.289341 , Training Accuracy =  0.25 Validation Accuracy =  0.2248\n",
      "Epoch:  04   =====> Loss = 2.280553 , Training Accuracy =  0.296875 Validation Accuracy =  0.2606\n",
      "Epoch:  05   =====> Loss = 2.267248 , Training Accuracy =  0.296875 Validation Accuracy =  0.3206\n",
      "Epoch:  07   =====> Loss = 2.193868 , Training Accuracy =  0.5078125 Validation Accuracy =  0.479\n",
      "Epoch:  08   =====> Loss = 2.064315 , Training Accuracy =  0.6171875 Validation Accuracy =  0.679\n",
      "Epoch:  09   =====> Loss = 1.687519 , Training Accuracy =  0.71875 Validation Accuracy =  0.7454\n",
      "Epoch:  10   =====> Loss = 1.077035 , Training Accuracy =  0.8125 Validation Accuracy =  0.7976\n",
      "Epoch:  11   =====> Loss = 0.708863 , Training Accuracy =  0.84375 Validation Accuracy =  0.8396\n",
      "Epoch:  12   =====> Loss = 0.549879 , Training Accuracy =  0.8984375 Validation Accuracy =  0.8686\n",
      "Epoch:  13   =====> Loss = 0.467807 , Training Accuracy =  0.84375 Validation Accuracy =  0.882\n",
      "Epoch:  14   =====> Loss = 0.416524 , Training Accuracy =  0.8671875 Validation Accuracy =  0.895\n",
      "Epoch:  15   =====> Loss = 0.381543 , Training Accuracy =  0.8515625 Validation Accuracy =  0.9014\n",
      "Epoch:  16   =====> Loss = 0.355428 , Training Accuracy =  0.9140625 Validation Accuracy =  0.9076\n",
      "Epoch:  17   =====> Loss = 0.335055 , Training Accuracy =  0.953125 Validation Accuracy =  0.9108\n",
      "Epoch:  18   =====> Loss = 0.318467 , Training Accuracy =  0.9609375 Validation Accuracy =  0.9146\n",
      "Epoch:  19   =====> Loss = 0.304296 , Training Accuracy =  0.9296875 Validation Accuracy =  0.9166\n",
      "Epoch:  20   =====> Loss = 0.291639 , Training Accuracy =  0.9453125 Validation Accuracy =  0.9202\n",
      "Epoch:  21   =====> Loss = 0.280831 , Training Accuracy =  0.890625 Validation Accuracy =  0.923\n",
      "Epoch:  22   =====> Loss = 0.270833 , Training Accuracy =  0.9453125 Validation Accuracy =  0.9258\n",
      "Epoch:  23   =====> Loss = 0.262194 , Training Accuracy =  0.921875 Validation Accuracy =  0.9278\n",
      "Epoch:  24   =====> Loss = 0.254076 , Training Accuracy =  0.8984375 Validation Accuracy =  0.929\n",
      "Epoch:  25   =====> Loss = 0.246292 , Training Accuracy =  0.8984375 Validation Accuracy =  0.9312\n",
      "Epoch:  26   =====> Loss = 0.239186 , Training Accuracy =  0.9296875 Validation Accuracy =  0.9324\n",
      "Epoch:  27   =====> Loss = 0.232914 , Training Accuracy =  0.953125 Validation Accuracy =  0.9346\n",
      "Epoch:  28   =====> Loss = 0.227011 , Training Accuracy =  0.90625 Validation Accuracy =  0.9366\n",
      "Epoch:  29   =====> Loss = 0.221186 , Training Accuracy =  0.921875 Validation Accuracy =  0.9386\n",
      "Epoch:  30   =====> Loss = 0.215571 , Training Accuracy =  0.9296875 Validation Accuracy =  0.9394\n",
      "Epoch:  31   =====> Loss = 0.210448 , Training Accuracy =  0.9140625 Validation Accuracy =  0.9404\n",
      "Epoch:  32   =====> Loss = 0.205351 , Training Accuracy =  0.9453125 Validation Accuracy =  0.9414\n",
      "Epoch:  33   =====> Loss = 0.200693 , Training Accuracy =  0.9375 Validation Accuracy =  0.9426\n",
      "Epoch:  34   =====> Loss = 0.196040 , Training Accuracy =  0.8984375 Validation Accuracy =  0.9434\n",
      "Epoch:  35   =====> Loss = 0.191735 , Training Accuracy =  0.9375 Validation Accuracy =  0.9466\n",
      "Epoch:  36   =====> Loss = 0.187426 , Training Accuracy =  0.9375 Validation Accuracy =  0.9476\n",
      "Epoch:  37   =====> Loss = 0.183464 , Training Accuracy =  0.953125 Validation Accuracy =  0.949\n",
      "Epoch:  38   =====> Loss = 0.179711 , Training Accuracy =  0.953125 Validation Accuracy =  0.9506\n",
      "Epoch:  39   =====> Loss = 0.175879 , Training Accuracy =  0.984375 Validation Accuracy =  0.9512\n",
      "Epoch:  40   =====> Loss = 0.172414 , Training Accuracy =  0.953125 Validation Accuracy =  0.9518\n",
      "Training time (Gradient Descent optimizer):  8568.230518102646 s\n",
      "Accuracy on test-set: 95.1%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "start = time.time()\n",
    "epochs = 40\n",
    "optimize2(epochs)\n",
    "print(\"Training time (Gradient Descent optimizer): \", time.time() - start, \"s\")\n",
    "\n",
    "print_accuracy2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put your loss and accuracy curves here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='inputs')\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "y_true_cls = tf.argmax(y_true, 1) \n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    logits = build_lenet5(x)   ## --- COMPLETE --- ##\n",
    "    y_pred = tf.nn.softmax(logits)   ## --- COMPLETE --- ##\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)   ## --- COMPLETE --- ##\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_true)   ## --- COMPLETE --- ##\n",
    "    loss = tf.reduce_mean(cross_entropy)   ## --- COMPLETE --- ##\n",
    "    \n",
    "with tf.name_scope('optim'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)   ## --- COMPLETE --- ##\n",
    "    opt_step = optimizer.minimize(loss)   ## --- COMPLETE --- ##\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)   ## --- COMPLETE --- ##\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   ## --- COMPLETE --- ##\n",
    "\n",
    "session = tf.Session()   ## --- COMPLETE --- ##\n",
    "init_op = tf.global_variables_initializer()   ## --- COMPLETE --- ##\n",
    "session.run(init_op)\n",
    "\n",
    "writer = tf.summary.FileWriter(next_path('logs/lenet5/run_%02d'))\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss = 0.319106 , Training Accuracy =  0.984375 , Validation Accuracy =  0.9684\n",
      "Epoch:  02   =====> Loss = 0.084518 , Training Accuracy =  1.0 , Validation Accuracy =  0.9776\n",
      "Epoch:  03   =====> Loss = 0.057625 , Training Accuracy =  0.9765625 , Validation Accuracy =  0.984\n",
      "Epoch:  04   =====> Loss = 0.042855 , Training Accuracy =  0.984375 , Validation Accuracy =  0.986\n",
      "Epoch:  05   =====> Loss = 0.033237 , Training Accuracy =  1.0 , Validation Accuracy =  0.9862\n",
      "Epoch:  06   =====> Loss = 0.026538 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.986\n",
      "Epoch:  07   =====> Loss = 0.021137 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.986\n",
      "Epoch:  08   =====> Loss = 0.016859 , Training Accuracy =  1.0 , Validation Accuracy =  0.9858\n",
      "Epoch:  09   =====> Loss = 0.014822 , Training Accuracy =  1.0 , Validation Accuracy =  0.9862\n",
      "Epoch:  10   =====> Loss = 0.012773 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9872\n",
      "Epoch:  11   =====> Loss = 0.010234 , Training Accuracy =  1.0 , Validation Accuracy =  0.9848\n",
      "Epoch:  12   =====> Loss = 0.009714 , Training Accuracy =  1.0 , Validation Accuracy =  0.9838\n",
      "Epoch:  13   =====> Loss = 0.007739 , Training Accuracy =  1.0 , Validation Accuracy =  0.9878\n",
      "Epoch:  14   =====> Loss = 0.006354 , Training Accuracy =  1.0 , Validation Accuracy =  0.9862\n",
      "Epoch:  15   =====> Loss = 0.005044 , Training Accuracy =  1.0 , Validation Accuracy =  0.9856\n",
      "Epoch:  16   =====> Loss = 0.004626 , Training Accuracy =  1.0 , Validation Accuracy =  0.9882\n",
      "Epoch:  17   =====> Loss = 0.004630 , Training Accuracy =  1.0 , Validation Accuracy =  0.9854\n",
      "Epoch:  18   =====> Loss = 0.003418 , Training Accuracy =  1.0 , Validation Accuracy =  0.9842\n",
      "Epoch:  19   =====> Loss = 0.002914 , Training Accuracy =  1.0 , Validation Accuracy =  0.9868\n",
      "Epoch:  20   =====> Loss = 0.004191 , Training Accuracy =  1.0 , Validation Accuracy =  0.99\n",
      "Epoch:  21   =====> Loss = 0.002832 , Training Accuracy =  1.0 , Validation Accuracy =  0.9896\n",
      "Epoch:  22   =====> Loss = 0.002318 , Training Accuracy =  1.0 , Validation Accuracy =  0.986\n",
      "Epoch:  23   =====> Loss = 0.001719 , Training Accuracy =  1.0 , Validation Accuracy =  0.9896\n",
      "Epoch:  24   =====> Loss = 0.001451 , Training Accuracy =  1.0 , Validation Accuracy =  0.9898\n",
      "Epoch:  25   =====> Loss = 0.001782 , Training Accuracy =  1.0 , Validation Accuracy =  0.9902\n",
      "Epoch:  26   =====> Loss = 0.002374 , Training Accuracy =  1.0 , Validation Accuracy =  0.9884\n",
      "Epoch:  27   =====> Loss = 0.001769 , Training Accuracy =  1.0 , Validation Accuracy =  0.9886\n",
      "Epoch:  28   =====> Loss = 0.001478 , Training Accuracy =  1.0 , Validation Accuracy =  0.9896\n",
      "Epoch:  29   =====> Loss = 0.001408 , Training Accuracy =  1.0 , Validation Accuracy =  0.991\n",
      "Epoch:  30   =====> Loss = 0.002113 , Training Accuracy =  1.0 , Validation Accuracy =  0.9886\n",
      "Epoch:  31   =====> Loss = 0.000954 , Training Accuracy =  1.0 , Validation Accuracy =  0.9906\n",
      "Epoch:  32   =====> Loss = 0.000851 , Training Accuracy =  1.0 , Validation Accuracy =  0.9912\n",
      "Epoch:  33   =====> Loss = 0.000979 , Training Accuracy =  1.0 , Validation Accuracy =  0.9878\n",
      "Epoch:  34   =====> Loss = 0.001453 , Training Accuracy =  1.0 , Validation Accuracy =  0.9908\n",
      "Epoch:  35   =====> Loss = 0.000783 , Training Accuracy =  1.0 , Validation Accuracy =  0.987\n",
      "Epoch:  36   =====> Loss = 0.000863 , Training Accuracy =  1.0 , Validation Accuracy =  0.9894\n",
      "Epoch:  37   =====> Loss = 0.000865 , Training Accuracy =  1.0 , Validation Accuracy =  0.9892\n",
      "Epoch:  38   =====> Loss = 0.001084 , Training Accuracy =  1.0 , Validation Accuracy =  0.9896\n",
      "Epoch:  39   =====> Loss = 0.000798 , Training Accuracy =  1.0 , Validation Accuracy =  0.9886\n",
      "Epoch:  40   =====> Loss = 0.001010 , Training Accuracy =  1.0 , Validation Accuracy =  0.9878\n",
      "Training time (Adam optimizer):  4595.790119171143 s\n",
      "Accuracy on test-set: 99.0%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "start = time.time()\n",
    "epochs = 40\n",
    "optimize2(epochs)\n",
    "print(\"Training time (Adam optimizer): \", time.time() - start, \"s\")\n",
    "\n",
    "print_accuracy2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |         95,1%      |        99,0%        |       \n",
    "| Training Time        |        8568s       |        4595s        |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "The <b> Adam optimizer </b> gives us, by far, the best accuracy on test data in less time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question </b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use `tf.nn.dropout` for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** On the testing data, we achieved <b> 1.0 accuracy </b>. However, the accuracy on the testing set is similar to the one obtained without dropout. It doesn't allow neither the algorithm to go faster. Thus, adding dropout is not essential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss = 0.406412 , Training Accuracy =  0.953125 , Validation Accuracy =  0.961\n",
      "Epoch:  02   =====> Loss = 0.111905 , Training Accuracy =  0.9765625 , Validation Accuracy =  0.9716\n",
      "Epoch:  03   =====> Loss = 0.076919 , Training Accuracy =  0.9765625 , Validation Accuracy =  0.9718\n",
      "Epoch:  04   =====> Loss = 0.060198 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.978\n",
      "Epoch:  05   =====> Loss = 0.053101 , Training Accuracy =  0.96875 , Validation Accuracy =  0.9822\n",
      "Epoch:  06   =====> Loss = 0.044314 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9814\n",
      "Epoch:  07   =====> Loss = 0.038939 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9828\n",
      "Epoch:  08   =====> Loss = 0.035548 , Training Accuracy =  0.9765625 , Validation Accuracy =  0.9864\n",
      "Epoch:  09   =====> Loss = 0.031038 , Training Accuracy =  0.984375 , Validation Accuracy =  0.987\n",
      "Epoch:  10   =====> Loss = 0.028235 , Training Accuracy =  0.984375 , Validation Accuracy =  0.9856\n",
      "Epoch:  11   =====> Loss = 0.025834 , Training Accuracy =  1.0 , Validation Accuracy =  0.9858\n",
      "Epoch:  12   =====> Loss = 0.025247 , Training Accuracy =  0.9765625 , Validation Accuracy =  0.9846\n",
      "Epoch:  13   =====> Loss = 0.021809 , Training Accuracy =  1.0 , Validation Accuracy =  0.986\n",
      "Epoch:  14   =====> Loss = 0.021670 , Training Accuracy =  1.0 , Validation Accuracy =  0.987\n",
      "Epoch:  15   =====> Loss = 0.019068 , Training Accuracy =  1.0 , Validation Accuracy =  0.9874\n",
      "Epoch:  16   =====> Loss = 0.017483 , Training Accuracy =  1.0 , Validation Accuracy =  0.9878\n",
      "Epoch:  17   =====> Loss = 0.016708 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.988\n",
      "Epoch:  18   =====> Loss = 0.015260 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9878\n",
      "Epoch:  19   =====> Loss = 0.014908 , Training Accuracy =  1.0 , Validation Accuracy =  0.9896\n",
      "Epoch:  20   =====> Loss = 0.014420 , Training Accuracy =  1.0 , Validation Accuracy =  0.9888\n",
      "Epoch:  21   =====> Loss = 0.014275 , Training Accuracy =  0.96875 , Validation Accuracy =  0.988\n",
      "Epoch:  22   =====> Loss = 0.013305 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9882\n",
      "Epoch:  23   =====> Loss = 0.012149 , Training Accuracy =  1.0 , Validation Accuracy =  0.9888\n",
      "Epoch:  24   =====> Loss = 0.012280 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9878\n",
      "Epoch:  25   =====> Loss = 0.011684 , Training Accuracy =  1.0 , Validation Accuracy =  0.988\n",
      "Epoch:  26   =====> Loss = 0.012009 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9878\n",
      "Epoch:  27   =====> Loss = 0.011282 , Training Accuracy =  1.0 , Validation Accuracy =  0.9892\n",
      "Epoch:  28   =====> Loss = 0.009908 , Training Accuracy =  1.0 , Validation Accuracy =  0.9856\n",
      "Epoch:  29   =====> Loss = 0.010356 , Training Accuracy =  1.0 , Validation Accuracy =  0.9868\n",
      "Epoch:  30   =====> Loss = 0.010649 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.988\n",
      "Epoch:  31   =====> Loss = 0.009764 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9888\n",
      "Epoch:  32   =====> Loss = 0.009690 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9886\n",
      "Epoch:  33   =====> Loss = 0.010441 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9868\n",
      "Epoch:  34   =====> Loss = 0.010902 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9878\n",
      "Epoch:  35   =====> Loss = 0.007658 , Training Accuracy =  1.0 , Validation Accuracy =  0.986\n",
      "Epoch:  36   =====> Loss = 0.009491 , Training Accuracy =  1.0 , Validation Accuracy =  0.9886\n",
      "Epoch:  37   =====> Loss = 0.008016 , Training Accuracy =  1.0 , Validation Accuracy =  0.9896\n",
      "Epoch:  38   =====> Loss = 0.009408 , Training Accuracy =  1.0 , Validation Accuracy =  0.9886\n",
      "Epoch:  39   =====> Loss = 0.009260 , Training Accuracy =  0.9921875 , Validation Accuracy =  0.9874\n",
      "Epoch:  40   =====> Loss = 0.008155 , Training Accuracy =  1.0 , Validation Accuracy =  0.9884\n",
      "Training time (Adam optimizer):  4731.569762468338 s\n",
      "Accuracy on test-set: 98.9%\n"
     ]
    }
   ],
   "source": [
    "def build_lenet5_dropout(x):    \n",
    "    ## --- COMPLETE --- ##\n",
    "    with tf.name_scope(\"reshape\"):\n",
    "        image = tf.reshape(x, [-1, 28, 28, 1]) # [None, 28, 28, 1]\n",
    "    \n",
    "    with tf.name_scope(\"lenet5\"):\n",
    "        with tf.name_scope(\"layer1\"):\n",
    "           ## --- COMPLETE --- ##\n",
    "            conv1_W = weight_variable((5, 5, 1, 6), \"conv1_W\")\n",
    "            conv1_b = bias_variable([1, 28, 28, 6], \"bias1\")\n",
    "            conv1 = tf.nn.conv2d(input = image, filter = conv1_W, strides = [1, 1, 1, 1], padding = 'SAME') + conv1_b\n",
    "            relu1 = tf.nn.relu(conv1)\n",
    "            max_pool1 = tf.nn.max_pool(value = relu1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"layer2\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            conv2_W = weight_variable((5, 5, 6, 16), \"conv2_W\")\n",
    "            conv2_b = bias_variable([1, 10, 10, 16], \"bias2\")\n",
    "            conv2 = tf.nn.conv2d(input = max_pool1, filter = conv2_W, strides = [1, 1, 1, 1], padding = 'VALID') + conv2_b\n",
    "            relu2 = tf.nn.relu(conv2)\n",
    "            max_pool2 = tf.nn.max_pool(value = relu2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"flatten\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            flat = flatten(max_pool2)\n",
    "            droupout = tf.nn.dropout(flat, keep_prob = 0.75)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"layer3\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            layer3_W = weight_variable((400, 120), \"layer3_W\")\n",
    "            layer3_b = tf.Variable(tf.zeros([120]), name='bias3')\n",
    "            fc3 = tf.matmul(droupout, layer3_W) + layer3_b\n",
    "            o_layer3 = tf.nn.relu(fc3)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"layer4\"):\n",
    "            ## --- COMPLETE --- ##    \n",
    "            layer4_W = weight_variable((120, 84), \"layer4_W\")\n",
    "            layer4_b = tf.Variable(tf.zeros([84]), name='bias4')\n",
    "            fc4 = tf.matmul(o_layer3, layer4_W) + layer4_b\n",
    "            o_layer4 = tf.nn.relu(fc4)\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"layer5\"):\n",
    "            ## --- COMPLETE --- ##\n",
    "            layer5_W = weight_variable((84, 10), \"layer5_W\")\n",
    "            layer5_b = tf.Variable(tf.zeros([10]), name='bias5')            \n",
    "            fc5 = tf.matmul(o_layer4, layer5_W) + layer5_b\n",
    "\n",
    "    \n",
    "    return fc5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "## --- COMPLETE --- ##\n",
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='inputs')\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "y_true_cls = tf.argmax(y_true, 1) \n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    logits = build_lenet5_dropout(x)   ## --- COMPLETE --- ##\n",
    "    y_pred = tf.nn.softmax(logits)   ## --- COMPLETE --- ##\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)   ## --- COMPLETE --- ##\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_true)   ## --- COMPLETE --- ##\n",
    "    loss = tf.reduce_mean(cross_entropy)   ## --- COMPLETE --- ##\n",
    "    \n",
    "with tf.name_scope('optim'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)   ## --- COMPLETE --- ##\n",
    "    opt_step = optimizer.minimize(loss)   ## --- COMPLETE --- ##\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)   ## --- COMPLETE --- ##\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   ## --- COMPLETE --- ##\n",
    "\n",
    "session = tf.Session()   ## --- COMPLETE --- ##\n",
    "init_op = tf.global_variables_initializer()   ## --- COMPLETE --- ##\n",
    "session.run(init_op)\n",
    "\n",
    "writer = tf.summary.FileWriter(next_path('logs/lenet5/run_%02d'))\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "epochs = 40\n",
    "optimize2(epochs)\n",
    "print(\"Training time (Adam optimizer): \", time.time() - start, \"s\")\n",
    "\n",
    "print_accuracy2()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": false,
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
